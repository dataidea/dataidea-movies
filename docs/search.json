[
  {
    "objectID": "fpl.html",
    "href": "fpl.html",
    "title": "Welcome to the DATAIDEA Fantasy Premier League",
    "section": "",
    "text": "Current Standings\n\n\n\n\n\n\n\n\n\nRank\nTeam & Manager\nGW\nTOT\n\n\n\n\n1\nBARÇA ya KAHUNGYE\n47\n2089\n\n\n2\nAŁbramo FC\n61\n2053\n\n\n3\nLethal Weapon\n53\n2009\n\n\n4\nBlyckfc\n30\n2002\n\n\n5\nBrine FC\n17\n1959\n\n\n6\nJ T\n78\n1958\n\n\n7\nJumaShafara@DATAIDEA\n43\n1951\n\n\n8\nRwenzori\n59\n1949\n\n\n9\nGem FC\n59\n1911\n\n\n10\nGULU UNITED F.C\n38\n1908\n\n\n11\nAJ12\n45\n1887\n\n\n12\nLegends FC\n44\n1823\n\n\n13\nM142 HIMARS\n56\n1804\n\n\n14\nColaz guluz\n42\n1781\n\n\n15\nPaperChaser of Uganda\n34\n1732\n\n\n16\nAFC Adventurer\n39\n1669\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\n Back to top"
  },
  {
    "objectID": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html",
    "href": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html",
    "title": "Handling Imbalanced Datasets",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nHandling imbalanced datasets is a common challenge in machine learning, especially in classification tasks where one class significantly outnumbers the other(s). Let’s go through a simple example using the popular Iris dataset, which we’ll artificially imbalance for demonstration purposes.\nThe Iris dataset consists of 150 samples, each belonging to one of three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. We’ll create an imbalanced version of this dataset where one class is underrepresented.\nFirst, let’s load the dataset and create the imbalance:\n# !pip install imbalanced-learn\n# !pip install --upgrade dataidea\nfrom dataidea.packages import pd, plt\nfrom sklearn.datasets import load_iris\n# Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Convert to DataFrame for manipulation\ndf = pd.DataFrame(data=np.c_[X, y],\n                  columns=iris.feature_names + ['target'])\n\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\ntarget\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n0.0\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n0.0\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n\n\n\n# Introduce imbalance by removing samples from one class\nclass_to_remove = 2  # Iris Virginica\nimbalance_ratio = 0.5  # Ratio of samples to be removed\nindices_to_remove = np.random.choice(df[df['target'] == class_to_remove].index,\n                                     size=int(imbalance_ratio * len(df[df['target'] == class_to_remove])),\n                                     replace=False)\ndf_imbalanced = df.drop(indices_to_remove)\n\n# Check the class distribution\nvalue_counts = df_imbalanced['target'].value_counts()\nprint(value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    25\nName: count, dtype: int64\nplt.bar(value_counts.index,\n        value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nNow, df_imbalanced contains the imbalanced dataset. Next, we’ll demonstrate a few techniques to handle this imbalance:\n\nResampling Methods:\n\nOversampling: Randomly duplicate samples from the minority class.\nUndersampling: Randomly remove samples from the majority class.\n\nSynthetic Sampling Methods:\n\nSMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class.\n\nAlgorithmic Techniques:\n\nAlgorithm Tuning: Adjusting class weights in the algorithm.\nEnsemble Methods: Using ensemble techniques like bagging or boosting."
  },
  {
    "objectID": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#handling-imbalanced-dataset",
    "href": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#handling-imbalanced-dataset",
    "title": "Handling Imbalanced Datasets",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nHandling imbalanced datasets is a common challenge in machine learning, especially in classification tasks where one class significantly outnumbers the other(s). Let’s go through a simple example using the popular Iris dataset, which we’ll artificially imbalance for demonstration purposes.\nThe Iris dataset consists of 150 samples, each belonging to one of three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. We’ll create an imbalanced version of this dataset where one class is underrepresented.\nFirst, let’s load the dataset and create the imbalance:\n# !pip install imbalanced-learn\n# !pip install --upgrade dataidea\nfrom dataidea.packages import pd, plt\nfrom sklearn.datasets import load_iris\n# Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Convert to DataFrame for manipulation\ndf = pd.DataFrame(data=np.c_[X, y],\n                  columns=iris.feature_names + ['target'])\n\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\ntarget\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n0.0\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n0.0\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n\n\n\n# Introduce imbalance by removing samples from one class\nclass_to_remove = 2  # Iris Virginica\nimbalance_ratio = 0.5  # Ratio of samples to be removed\nindices_to_remove = np.random.choice(df[df['target'] == class_to_remove].index,\n                                     size=int(imbalance_ratio * len(df[df['target'] == class_to_remove])),\n                                     replace=False)\ndf_imbalanced = df.drop(indices_to_remove)\n\n# Check the class distribution\nvalue_counts = df_imbalanced['target'].value_counts()\nprint(value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    25\nName: count, dtype: int64\nplt.bar(value_counts.index,\n        value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nNow, df_imbalanced contains the imbalanced dataset. Next, we’ll demonstrate a few techniques to handle this imbalance:\n\nResampling Methods:\n\nOversampling: Randomly duplicate samples from the minority class.\nUndersampling: Randomly remove samples from the majority class.\n\nSynthetic Sampling Methods:\n\nSMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class.\n\nAlgorithmic Techniques:\n\nAlgorithm Tuning: Adjusting class weights in the algorithm.\nEnsemble Methods: Using ensemble techniques like bagging or boosting."
  },
  {
    "objectID": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#resampling",
    "href": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#resampling",
    "title": "Handling Imbalanced Datasets",
    "section": "Resampling",
    "text": "Resampling\n\nOversampling\nLet’s implement oversampling using the imbalanced-learn library:\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Separate features and target\nX_imbalanced = df_imbalanced.drop('target', axis=1)\ny_imbalanced = df_imbalanced['target']\n\n# Apply Random Over-Sampling\noversample = RandomOverSampler(sampling_strategy='auto',\n                               random_state=42)\nX_resampled, y_resampled = oversample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after oversampling\noversampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(oversampled_data_value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\nplt.bar(oversampled_data_value_counts.index,\n        oversampled_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\n\n\nUndersampling:\nUndersampling involves reducing the number of samples in the majority class to balance the dataset. Here’s how you can apply random undersampling using the imbalanced-learn library:\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Apply Random Under-Sampling\nundersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = undersample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after undersampling\nundersampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(undersampled_data_value_counts)\ntarget\n0.0    25\n1.0    25\n2.0    25\nName: count, dtype: int64\nplt.bar(undersampled_data_value_counts.index,\n        undersampled_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#smote-synthetic-minority-over-sampling-technique",
    "href": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#smote-synthetic-minority-over-sampling-technique",
    "title": "Handling Imbalanced Datasets",
    "section": "SMOTE (Synthetic Minority Over-sampling Technique)",
    "text": "SMOTE (Synthetic Minority Over-sampling Technique)\nSMOTE generates synthetic samples for the minority class by interpolating between existing minority class samples.\nHere’s how you can apply SMOTE using the imbalanced-learn library:\nfrom imblearn.over_sampling import SMOTE\n\n# Apply SMOTE\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after SMOTE\nsmoted_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(smoted_data_value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\nplt.bar(smoted_data_value_counts.index,\n        smoted_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nBy using SMOTE, you can generate synthetic samples for the minority class, effectively increasing its representation in the dataset. This can help to mitigate the class imbalance issue and improve the performance of your machine learning model."
  },
  {
    "objectID": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#algorithmic-techniques",
    "href": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#algorithmic-techniques",
    "title": "Handling Imbalanced Datasets",
    "section": "Algorithmic Techniques",
    "text": "Algorithmic Techniques\n\nAlgorithm Tuning:\nMany algorithms allow you to specify class weights to penalize misclassifications of the minority class more heavily. Here’s an example using the class_weight parameter in a logistic regression classifier:\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced,\n                                                    test_size=0.2, random_state=42)\n\n# Define the logistic regression classifier with class weights\nclass_weights = {0: 1, 1: 1, 2: 20}  # Penalize the minority class more heavily\nlog_reg = LogisticRegression(class_weight=class_weights)\n\n# Train the model\nlog_reg.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = log_reg.predict(X_test)\n\n# display classification report\npd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.750000\n\n\n0.857143\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.666667\n\n\n1.000000\n\n\n0.800000\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.920000\n\n\n0.920000\n\n\n0.920000\n\n\n0.92\n\n\n\n\nmacro avg\n\n\n0.888889\n\n\n0.916667\n\n\n0.885714\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.946667\n\n\n0.920000\n\n\n0.922286\n\n\n25.00\n\n\n\n\n\nIn this example, the class weight for the minority class is increased to penalize misclassifications more heavily.\n\n\nEnsemble Methods\nEnsemble methods can also be effective for handling imbalanced datasets. Techniques such as bagging and boosting can improve the performance of classifiers, especially when dealing with imbalanced classes.\nHere’s an example of using ensemble methods like Random Forest, a popular bagging algorithm, with an imbalanced dataset:\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Define and train Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"Random Forest Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_rf, output_dict=True)).transpose()\nRandom Forest Classifier:\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.875000\n\n\n0.933333\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.800000\n\n\n1.000000\n\n\n0.888889\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.960000\n\n\n0.960000\n\n\n0.960000\n\n\n0.96\n\n\n\n\nmacro avg\n\n\n0.933333\n\n\n0.958333\n\n\n0.940741\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.968000\n\n\n0.960000\n\n\n0.960889\n\n\n25.00\n\n\n\n\n\nEnsemble methods like Random Forest build multiple decision trees and combine their predictions to make a final prediction. This can often lead to better generalization and performance, even in the presence of imbalanced classes.\n\n\nAdaBoost Classifier\nAnother ensemble method that specifically addresses class imbalance is AdaBoost (Adaptive Boosting). AdaBoost focuses more on those training instances that were previously misclassified, thus giving higher weight to the minority class instances.\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Define and train AdaBoost classifier\nada_classifier = AdaBoostClassifier(n_estimators=100, random_state=42)\nada_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_ada = ada_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"AdaBoost Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_ada, output_dict=True)).transpose()\nAdaBoost Classifier:\n\n\n/home/jumashafara/venvs/dataidea/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n  warnings.warn(\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.875000\n\n\n0.933333\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.800000\n\n\n1.000000\n\n\n0.888889\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.960000\n\n\n0.960000\n\n\n0.960000\n\n\n0.96\n\n\n\n\nmacro avg\n\n\n0.933333\n\n\n0.958333\n\n\n0.940741\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.968000\n\n\n0.960000\n\n\n0.960889\n\n\n25.00\n\n\n\n\n\nBy utilizing ensemble methods like Random Forest and AdaBoost, you can often achieve better performance on imbalanced datasets compared to individual classifiers, as these methods inherently mitigate the effects of class imbalance through their construction.\nThese are just a few techniques for handling imbalanced datasets. It’s crucial to experiment with different methods and evaluate their performance using appropriate evaluation metrics to find the best approach for your specific problem.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/time_series_analysis/index.html",
    "href": "posts/time_series_analysis/index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 2 of our Time Series Analysis Series, Part 1 introduces you to what time series is. The link to part 1 is here\nAs name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.\n\n\nWe have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.\n[ad]"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#time-series-analysis",
    "href": "posts/time_series_analysis/index.html#time-series-analysis",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 2 of our Time Series Analysis Series, Part 1 introduces you to what time series is. The link to part 1 is here\nAs name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.\n\n\nWe have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.\n[ad]"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#decomposition-of-time-series",
    "href": "posts/time_series_analysis/index.html#decomposition-of-time-series",
    "title": "Time Series Analysis",
    "section": "Decomposition of Time Series",
    "text": "Decomposition of Time Series\nTime series decomposition helps to deconstruct the time series into several component like trend and seasonality for better visualization of its characteristics. Using time-series decomposition makes it easier to quickly identify a changing mean or variation in the data\n\n\n\nDecomposition_of_Time_Series"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#stationary-data",
    "href": "posts/time_series_analysis/index.html#stationary-data",
    "title": "Time Series Analysis",
    "section": "Stationary Data",
    "text": "Stationary Data\nFor accurate analysis and forecasting trend and seasonality is removed from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time.\n\n\n\nStationarity"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#test-for-stationarity",
    "href": "posts/time_series_analysis/index.html#test-for-stationarity",
    "title": "Time Series Analysis",
    "section": "Test for Stationarity",
    "text": "Test for Stationarity\nEasy way is to look at the plot and look for any obvious trend or seasonality. While working on real world data we can also use more sophisticated methods like rolling statistic and Augmented Dickey Fuller test to check stationarity of the data.\n\nRolling Statistics\nIn rolling statistics technique we define a size of window to calculate the mean and standard deviation throughout the series. For stationary series mean and standard deviation shouldn’t change with time.\n\n\nAugmented Dickey Fuller (ADF) Test\nI won’t go into the details of how this test works. I will concentrate more on how to interpret the result of this test to determine the stationarity of the series. ADF test will return ‘p-value’ and ‘Test Statistics’ output values.\n\np-value &gt; 0.05: non-stationary.\np-value &lt;= 0.05: stationary.\nTest statistics: More negative this value more likely we have stationary series. Also, this value should be smaller than critical values(1%, 5%, 10%). For e.g. If test statistic is smaller than the 5% critical values, then we can say with 95% confidence that this is a stationary series\n\n[ad]"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#convert-non-stationary-data-to-stationary-data",
    "href": "posts/time_series_analysis/index.html#convert-non-stationary-data-to-stationary-data",
    "title": "Time Series Analysis",
    "section": "Convert Non-Stationary Data to Stationary Data",
    "text": "Convert Non-Stationary Data to Stationary Data\nAccounting for the time series data characteristics like trend and seasonality is called as making data stationary. So by making the mean and variance of the time series constant, we will get the stationary data. Below are the few technique used for the same…\n\nDifferencing\nDifferencing technique helps to remove the trend and seasonality from time series data. Differencing is performed by subtracting the previous observation from the current observation. The differenced data will contain one less data point than original data. So differencing actually reduces the number of observations and stabilize the mean of a time series.\n\\[d = t - t0\\]\nAfter performing the differencing it’s recommended to plot the data and visualize the change. In case there is not sufficient improvement you can perform second order or even third order differencing.\n\n\nTransformation\nA simple but often effective way to stabilize the variance across time is to apply a power transformation to the time series. Log, square root, cube root are most commonly used transformation techniques. Most of the time you can pick the type of growth of the time series and accordingly choose the transformation method. For. e.g. A time series that has a quadratic growth trend can be made linear by taking the square root. In case differencing don’t work, you may first want to use one of above transformation technique to remove the variation from the series.\n\n\n\nLog_Transformation\n\n\n\n\nMoving Average\nIn moving averages technique, a new series is created by taking the averages of data points from original series. In this technique we can use two or more raw data points to calculate the average. This is also called as ‘window width (w)’. Once window width is decided, averages are calculated from start to the end for each set of w consecutive values, hence the name moving averages. It can also be used for time series forecasting.\n\n\n\nMoving_Average\n\n\n\nWeighted Moving Averages(WMA)\nWMA is a technical indicator that assigns a greater weighting to the most recent data points, and less weighting to data points in the distant past. The WMA is obtained by multiplying each number in the data set by a predetermined weight and summing up the resulting values. There can be many techniques for assigning weights. A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor.\n\n\nCentered Moving Averages(CMS)\nIn a centered moving average, the value of the moving average at time t is computed by centering the window around time t and averaging across the w values within the window. For example, a center moving average with a window of 3 would be calculated as\n\\[CMA(t) = mean(t-1, t, t+1)\\]\nCMA is very useful for visualizing the time series data\n\n\nTrailing Moving Averages(TMA)\nIn trailing moving average, instead of averaging over a window that is centered around a time period of interest, it simply takes the average of the last w values. For example, a trailing moving average with a window of 3 would be calculated as:\n\\[TMA(t) = mean(t-2, t-1, t)\\]\nTMA are useful for forecasting.\n[ad]"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#correlation",
    "href": "posts/time_series_analysis/index.html#correlation",
    "title": "Time Series Analysis",
    "section": "Correlation",
    "text": "Correlation\n\nMost important point about values in time series is its dependence on the previous values.\nWe can calculate the correlation for time series observations with previous time steps, called as lags.\nBecause the correlation of the time series observations is calculated with values of the same series at previous times, this is called an autocorrelation or serial correlation.\nTo understand it better lets consider the example of fish prices. We will use below notation to represent the fish prices.\n\n\\(P(t)\\)= Fish price of today\n\\(P(t-1)\\) = Fish price of last month\n\\(P(t-2)\\) =Fish price of last to last month\n\nTime series of fish prices can be represented as \\(P(t-n),..... P(t-3), P(t-2),P(t-1), P(t)\\)\nSo if we have fish prices for last few months then it will be easy for us to predict the fish price for today (Here we are ignoring all other external factors that may affect the fish prices)\n\nAll the past and future data points are related in time series and ACF and PACF functions help us to determine correlation in it.\n\nAuto Correlation Function (ACF)\n\nACF tells you how correlated points are with each other, based on how many time steps they are separated by.\nNow to understand it better lets consider above example of fish prices. Let’s try to find the correlation between fish price for current month P(t) and two months ago P(t-2). Important thing to note that, fish price of two months ago can directly affect the today’s fish price or it can indirectly affect the fish price through last months price P(t-1)\nSo ACF consider the direct as well indirect effect between the points while determining the correlation\n\n\n\nPartial Auto Correlation Function (PACF)\n\nUnlike ACF, PACF only consider the direct effect between the points while determining the correlation\nIn case of above fish price example PACF will determine the correlation between fish price for current month P(t) and two months ago P(t-2) by considering only P(t) and P(t-2) and ignoring P(t-1)\n\n[ad]"
  },
  {
    "objectID": "posts/ml_model_deployment/index.html",
    "href": "posts/ml_model_deployment/index.html",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models is a crucial step in turning your data science projects into practical, real-world applications. With a variety of deployment options available, it’s essential to choose the one that best fits your needs. In this blog, we’ll explore the different deployment options and help you determine the best approach for your machine learning models.\n\n\nCloud platforms provide scalable and flexible environments for deploying machine learning models, making them a popular choice. Here’s a look at some of the leading cloud services:\nAmazon Web Services (AWS):\n\nSageMaker: A comprehensive service that offers tools to build, train, and deploy models at scale.\nLambda: Ideal for deploying lightweight models that need to respond to API requests.\nEC2 Instances: Customizable virtual servers suitable for deploying larger, more complex models.\n\nGoogle Cloud Platform (GCP):\n\nAI Platform: An integrated toolset designed for deploying and managing models efficiently.\nCloud Functions: A serverless environment perfect for lightweight model deployments.\nCompute Engine: Scalable virtual machines for more extensive and customized deployments.\n\nMicrosoft Azure:\n\nAzure Machine Learning: An end-to-end platform for building, training, and deploying models.\nAzure Functions: Serverless compute service ideal for deploying lightweight models.\nVirtual Machines: Provide a custom deployment environment tailored to your needs.\n\n\n\n\nFor organizations that require control over their infrastructure due to security, compliance, or performance needs, on-premises deployment is an excellent choice. Here are some options:\n\nDocker Containers: Ensures consistency across different environments by containerizing models.\nKubernetes: Manages and orchestrates containerized applications, offering scaling and management capabilities.\nDedicated Hardware: Utilizes specific hardware like GPUs or TPUs for high-performance requirements.\n\n\n\n\nEdge deployment is perfect for real-time processing and reduced latency by deploying models on edge devices like IoT devices and mobile phones. Some popular solutions include:\n\nTensorFlow Lite: Deploys TensorFlow models on mobile and embedded devices.\nONNX Runtime: Runs models trained in various frameworks on edge devices.\nNVIDIA Jetson: A platform for deploying AI models on edge devices with GPU acceleration.\n\n\n\n\nHybrid deployment combines multiple environments to leverage the advantages of each. For example, you might use the cloud for training and initial deployment but edge devices for inference to minimize latency and bandwidth usage.\n\n\n\nServerless computing allows you to deploy models without managing the underlying infrastructure. This option is ideal for applications with variable traffic and includes services like:\n\nAWS Lambda\nGoogle Cloud Functions\nAzure Functions\n\nThese services automatically scale with the load, simplifying the deployment process.\n\n\n\nExposing your model as a web service via RESTful or GraphQL APIs is a flexible and widely used approach. This allows various applications and services to access your model via HTTP requests. Some popular frameworks include:\n\nFlask/Django: Web application frameworks for Python.\nFastAPI: A modern, high-performance web framework for building APIs with Python 3.7+.\nTensorFlow Serving: Serves TensorFlow models as APIs, streamlining deployment.\n\n\n\n\nWhen real-time inference isn’t necessary, batch processing is a great option for handling large volumes of data at scheduled intervals. Key tools include:\n\nApache Spark: A unified analytics engine for large-scale data processing.\nHadoop: A framework for distributed storage and processing of big data."
  },
  {
    "objectID": "posts/ml_model_deployment/index.html#exploring-options-for-deploying-machine-learning-models",
    "href": "posts/ml_model_deployment/index.html#exploring-options-for-deploying-machine-learning-models",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models is a crucial step in turning your data science projects into practical, real-world applications. With a variety of deployment options available, it’s essential to choose the one that best fits your needs. In this blog, we’ll explore the different deployment options and help you determine the best approach for your machine learning models.\n\n\nCloud platforms provide scalable and flexible environments for deploying machine learning models, making them a popular choice. Here’s a look at some of the leading cloud services:\nAmazon Web Services (AWS):\n\nSageMaker: A comprehensive service that offers tools to build, train, and deploy models at scale.\nLambda: Ideal for deploying lightweight models that need to respond to API requests.\nEC2 Instances: Customizable virtual servers suitable for deploying larger, more complex models.\n\nGoogle Cloud Platform (GCP):\n\nAI Platform: An integrated toolset designed for deploying and managing models efficiently.\nCloud Functions: A serverless environment perfect for lightweight model deployments.\nCompute Engine: Scalable virtual machines for more extensive and customized deployments.\n\nMicrosoft Azure:\n\nAzure Machine Learning: An end-to-end platform for building, training, and deploying models.\nAzure Functions: Serverless compute service ideal for deploying lightweight models.\nVirtual Machines: Provide a custom deployment environment tailored to your needs.\n\n\n\n\nFor organizations that require control over their infrastructure due to security, compliance, or performance needs, on-premises deployment is an excellent choice. Here are some options:\n\nDocker Containers: Ensures consistency across different environments by containerizing models.\nKubernetes: Manages and orchestrates containerized applications, offering scaling and management capabilities.\nDedicated Hardware: Utilizes specific hardware like GPUs or TPUs for high-performance requirements.\n\n\n\n\nEdge deployment is perfect for real-time processing and reduced latency by deploying models on edge devices like IoT devices and mobile phones. Some popular solutions include:\n\nTensorFlow Lite: Deploys TensorFlow models on mobile and embedded devices.\nONNX Runtime: Runs models trained in various frameworks on edge devices.\nNVIDIA Jetson: A platform for deploying AI models on edge devices with GPU acceleration.\n\n\n\n\nHybrid deployment combines multiple environments to leverage the advantages of each. For example, you might use the cloud for training and initial deployment but edge devices for inference to minimize latency and bandwidth usage.\n\n\n\nServerless computing allows you to deploy models without managing the underlying infrastructure. This option is ideal for applications with variable traffic and includes services like:\n\nAWS Lambda\nGoogle Cloud Functions\nAzure Functions\n\nThese services automatically scale with the load, simplifying the deployment process.\n\n\n\nExposing your model as a web service via RESTful or GraphQL APIs is a flexible and widely used approach. This allows various applications and services to access your model via HTTP requests. Some popular frameworks include:\n\nFlask/Django: Web application frameworks for Python.\nFastAPI: A modern, high-performance web framework for building APIs with Python 3.7+.\nTensorFlow Serving: Serves TensorFlow models as APIs, streamlining deployment.\n\n\n\n\nWhen real-time inference isn’t necessary, batch processing is a great option for handling large volumes of data at scheduled intervals. Key tools include:\n\nApache Spark: A unified analytics engine for large-scale data processing.\nHadoop: A framework for distributed storage and processing of big data."
  },
  {
    "objectID": "posts/ml_model_deployment/index.html#key-considerations-for-model-deployment",
    "href": "posts/ml_model_deployment/index.html#key-considerations-for-model-deployment",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "Key Considerations for Model Deployment",
    "text": "Key Considerations for Model Deployment\nWhen deciding on a deployment option, keep these factors in mind:\n\nScalability: Can the solution handle the expected load?\nLatency: Is real-time inference required?\nCost: What are the costs associated with running the model in production?\nSecurity: Does the deployment meet your security and compliance requirements?\nEase of Use: How straightforward is it to deploy, manage, and update the model?\nIntegration: How well does the deployment option integrate with your existing systems and workflows?\n\nSelecting the right deployment option is essential for balancing performance, cost, and operational complexity. By carefully considering your specific requirements and constraints, you can ensure a successful deployment of your machine learning models.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/budget_ml_deploy_options/index.html",
    "href": "posts/budget_ml_deploy_options/index.html",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models doesn’t have to break the bank. If you’re working within a budget, there are several cost-effective options available that still provide robust capabilities. In this blog, we’ll explore some of the best budget-friendly deployment strategies for machine learning models.\n\n\nServerless computing is a great way to minimize costs because you only pay for what you use, without worrying about managing infrastructure. Here are some economical serverless options:\nAWS Lambda:\n\nCost: Pay-per-request pricing.\nPros: Automatically scales with demand, integrates well with other AWS services.\nCons: Limited to short-duration tasks and smaller memory limits.\n\nGoogle Cloud Functions:\n\nCost: Pay-per-use based on the number of invocations, compute time, and memory allocated.\nPros: Excellent for event-driven architectures, integrates smoothly with other Google Cloud services.\nCons: Similar constraints on execution time and memory as AWS Lambda.\n\nAzure Functions:\n\nCost: Pay-per-execution pricing model.\nPros: Scalable and integrates with a wide range of Azure services.\nCons: Limited by the maximum execution duration and resource allocation.\n\n\n\n\nContainers can be a cost-effective way to deploy models by ensuring efficient use of resources and consistent performance across different environments.\nDocker:\n\nCost: Free to use; additional costs come from the underlying infrastructure you deploy it on.\nPros: Lightweight, portable, and scalable. Works well with various cloud and on-premises environments.\nCons: Requires some setup and management expertise.\n\nKubernetes (K8s):\n\nCost: Free, but incurs costs related to the infrastructure on which it runs.\nPros: Manages containerized applications at scale, great for complex deployments.\nCons: More complex setup and management compared to Docker alone.\n\n\n\n\nPaaS solutions can reduce operational overhead and are generally cost-effective for small to medium-scale deployments.\nGoogle App Engine:\n\nCost: Offers a free tier and then pay-as-you-go pricing.\nPros: Fully managed, scalable service for web applications and APIs.\nCons: Less control over the underlying infrastructure.\n\nHeroku:\n\nCost: Free tier available, with affordable pricing plans for higher usage.\nPros: Easy to use, supports multiple programming languages, and integrates with various databases.\nCons: Might become expensive as you scale up.\n\n\n\n\nIf you need more control over your deployment environment but still want to keep costs low, consider budget-friendly cloud services:\nDigitalOcean:\n\nCost: Very competitive pricing with flexible plans starting as low as $5 per month.\nPros: Simple to set up, provides good performance for the price, supports Docker and Kubernetes.\nCons: Fewer advanced features compared to larger cloud providers.\n\nLinode:\n\nCost: Affordable plans starting at $5 per month.\nPros: Good performance, simple pricing structure, supports various deployment configurations.\nCons: Smaller ecosystem and fewer services compared to larger cloud providers.\n\nVultr:\n\nCost: Plans also starting at $5 per month.\nPros: Wide range of instances, including high-frequency compute instances.\nCons: Similar to DigitalOcean and Linode in terms of limitations.\n\n\n\n\nFor specific use cases where you need low latency and reduced data transfer costs, edge deployment can be both effective and economical:\nRaspberry Pi:\n\nCost: Low-cost hardware starting around $35.\nPros: Great for small-scale deployments, supports various machine learning frameworks.\nCons: Limited computational power, suitable for less intensive tasks.\n\nNVIDIA Jetson Nano:\n\nCost: Around $99.\nPros: Affordable, powerful enough for real-time AI tasks on edge devices.\nCons: Higher cost than Raspberry Pi but offers better performance."
  },
  {
    "objectID": "posts/budget_ml_deploy_options/index.html#budget-friendly-options-for-deploying-machine-learning-models",
    "href": "posts/budget_ml_deploy_options/index.html#budget-friendly-options-for-deploying-machine-learning-models",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models doesn’t have to break the bank. If you’re working within a budget, there are several cost-effective options available that still provide robust capabilities. In this blog, we’ll explore some of the best budget-friendly deployment strategies for machine learning models.\n\n\nServerless computing is a great way to minimize costs because you only pay for what you use, without worrying about managing infrastructure. Here are some economical serverless options:\nAWS Lambda:\n\nCost: Pay-per-request pricing.\nPros: Automatically scales with demand, integrates well with other AWS services.\nCons: Limited to short-duration tasks and smaller memory limits.\n\nGoogle Cloud Functions:\n\nCost: Pay-per-use based on the number of invocations, compute time, and memory allocated.\nPros: Excellent for event-driven architectures, integrates smoothly with other Google Cloud services.\nCons: Similar constraints on execution time and memory as AWS Lambda.\n\nAzure Functions:\n\nCost: Pay-per-execution pricing model.\nPros: Scalable and integrates with a wide range of Azure services.\nCons: Limited by the maximum execution duration and resource allocation.\n\n\n\n\nContainers can be a cost-effective way to deploy models by ensuring efficient use of resources and consistent performance across different environments.\nDocker:\n\nCost: Free to use; additional costs come from the underlying infrastructure you deploy it on.\nPros: Lightweight, portable, and scalable. Works well with various cloud and on-premises environments.\nCons: Requires some setup and management expertise.\n\nKubernetes (K8s):\n\nCost: Free, but incurs costs related to the infrastructure on which it runs.\nPros: Manages containerized applications at scale, great for complex deployments.\nCons: More complex setup and management compared to Docker alone.\n\n\n\n\nPaaS solutions can reduce operational overhead and are generally cost-effective for small to medium-scale deployments.\nGoogle App Engine:\n\nCost: Offers a free tier and then pay-as-you-go pricing.\nPros: Fully managed, scalable service for web applications and APIs.\nCons: Less control over the underlying infrastructure.\n\nHeroku:\n\nCost: Free tier available, with affordable pricing plans for higher usage.\nPros: Easy to use, supports multiple programming languages, and integrates with various databases.\nCons: Might become expensive as you scale up.\n\n\n\n\nIf you need more control over your deployment environment but still want to keep costs low, consider budget-friendly cloud services:\nDigitalOcean:\n\nCost: Very competitive pricing with flexible plans starting as low as $5 per month.\nPros: Simple to set up, provides good performance for the price, supports Docker and Kubernetes.\nCons: Fewer advanced features compared to larger cloud providers.\n\nLinode:\n\nCost: Affordable plans starting at $5 per month.\nPros: Good performance, simple pricing structure, supports various deployment configurations.\nCons: Smaller ecosystem and fewer services compared to larger cloud providers.\n\nVultr:\n\nCost: Plans also starting at $5 per month.\nPros: Wide range of instances, including high-frequency compute instances.\nCons: Similar to DigitalOcean and Linode in terms of limitations.\n\n\n\n\nFor specific use cases where you need low latency and reduced data transfer costs, edge deployment can be both effective and economical:\nRaspberry Pi:\n\nCost: Low-cost hardware starting around $35.\nPros: Great for small-scale deployments, supports various machine learning frameworks.\nCons: Limited computational power, suitable for less intensive tasks.\n\nNVIDIA Jetson Nano:\n\nCost: Around $99.\nPros: Affordable, powerful enough for real-time AI tasks on edge devices.\nCons: Higher cost than Raspberry Pi but offers better performance."
  },
  {
    "objectID": "posts/budget_ml_deploy_options/index.html#key-tips-for-budget-friendly-deployment",
    "href": "posts/budget_ml_deploy_options/index.html#key-tips-for-budget-friendly-deployment",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "Key Tips for Budget-Friendly Deployment",
    "text": "Key Tips for Budget-Friendly Deployment\n\nOptimize Your Model: Smaller and optimized models can reduce computational requirements and, consequently, deployment costs.\nLeverage Free Tiers: Many cloud providers offer free tiers or credits for new users; take advantage of these offers.\nAuto-scaling: Use auto-scaling features to ensure you’re only paying for the resources you need at any given time.\nMonitor and Optimize: Regularly monitor resource usage and optimize configurations to avoid unnecessary costs.\nUse Spot Instances: For non-critical workloads, consider using spot instances, which are significantly cheaper than regular instances.\n\nBy carefully selecting your deployment strategy and optimizing your resources, you can deploy machine learning models effectively without exceeding your budget.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/creating_venvs/index.html",
    "href": "posts/creating_venvs/index.html",
    "title": "Python Virtual Environments",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nIn the vast ecosystem of Python programming, managing dependencies can sometimes be a daunting task. As projects grow in complexity, so does the need for a clean and isolated environment where dependencies can be installed without affecting other projects or the system-wide Python installation. This is where Python virtual environments come into play. In this guide, we’ll walk through the process of creating a virtual environment using venv and installing a package, like DataIdea, within it.\n\nWhat is a Python Virtual Environment?\nA virtual environment is a self-contained directory tree that contains a Python installation for a particular version of Python, plus a number of additional packages. It allows you to work on a Python project in isolation, ensuring that your project’s dependencies are maintained separately from other projects or the system Python.\n\n\nStep 1: Setting Up a Virtual Environment\nPython 3 comes with a built-in module called venv, which is used to create virtual environments. To create a virtual environment, open your terminal or command prompt and navigate to the directory where you want to create the environment. Then, run the following command:\n\nOn macOS and Linux:\n\npython3 -m venv myenv\n\nOn Windows:\n\npython -m venv myenv\nReplace myenv with the name you want to give to your virtual environment. This command will create a directory named myenv (or whatever name you provided) containing a Python interpreter and other necessary files.\n\n\n\n\n\n\n\nStep 2: Activating the Virtual Environment\nOnce the virtual environment is created, you need to activate it. Activation sets up the environment variables and modifies your shell prompt to indicate that you are now working within the virtual environment. Activate the virtual environment by running the appropriate command for your operating system:\n\nOn macOS and Linux:\n\nsource myenv/bin/activate\n\nOn Windows:\n\nmyenv\\Scripts\\activate\nYou’ll notice that your command prompt changes to show the name of the activated virtual environment.\n\n\n\n\n\n\n\nStep 3: Installing Packages\nWith the virtual environment activated, you can now install packages without affecting the global Python installation. Let’s install dataidea, as an example:\npip install dataidea\nyou can replace dataidea with another name of the package you want to install.\n\n\nStep 4: Using dataidea in Your Project\nOnce the package is installed, you can start using it in your Python project. Simply import it in your Python scripts as you would with any other package:\nimport dataidea\n\n\n\n\n\n\n\nStep 5: Deactivating the Virtual Environment\nWhen you’re done working on your project and want to leave the virtual environment, you can deactivate it by simply typing:\ndeactivate\n\n\nConclusion\nPython virtual environments are indispensable tools for managing dependencies and keeping project environments clean and isolated. With the venv module, creating and managing virtual environments has become easier than ever. By following the steps outlined in this guide, you can create a virtual environment, install packages like DataIdea, and develop Python projects with confidence. Happy coding!\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html",
    "href": "posts/mr_robot_softwares/index.html",
    "title": "All Software used in Mr. Robot",
    "section": "",
    "text": "Thumbnail\nHere’s a list of the most popular softwares used by the hackers in the Emmy and Golden Globe award winning drama/thriller series Mr. Robot."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#kali-linux",
    "href": "posts/mr_robot_softwares/index.html#kali-linux",
    "title": "All Software used in Mr. Robot",
    "section": "Kali Linux",
    "text": "Kali Linux\nKali Linux is a Linux distro made for security researchers for penetration testing, but is also used by hackers since it is jam packed with hacking tools. It is regularly featured in Mr. Robot since it is the hackers’ operating system of choice."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#tor-browser",
    "href": "posts/mr_robot_softwares/index.html#tor-browser",
    "title": "All Software used in Mr. Robot",
    "section": "Tor Browser",
    "text": "Tor Browser\nTor Browser is widely considered to be the best anonymizing tool out there. It will make your Internet activity very hard to trace, which fsociety takes advantage of when Trenton in season 2, episode 8 uploads a leaked FBI conference call about illegal mass surveillance to Vimeo using Tor Browser."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#raspberry-pi",
    "href": "posts/mr_robot_softwares/index.html#raspberry-pi",
    "title": "All Software used in Mr. Robot",
    "section": "Raspberry Pi",
    "text": "Raspberry Pi\nRaspberry Pi is a small, programmable computer board designed to teach children about computer science. It is also favourite among hobbyists and programmers due to its low-cost, versatility and simplicity. In season 1, episode 5 Elliot installs a Raspberry Pi into Steel Mountain’s climate control system so that fsociety at a later point in time can remotely raise the temperature in the storage room where Evil Corp’s tape backups are stored, resulting in the backups of the records of a significant portion of the US’ consumer debt being destroyed."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#proton-mail",
    "href": "posts/mr_robot_softwares/index.html#proton-mail",
    "title": "All Software used in Mr. Robot",
    "section": "Proton Mail",
    "text": "Proton Mail\nProtonMail is a secure, end-to-end encrypted e-mail service based in Switzerland that is used by Elliot in season 1, episode 8. The team behind Mr. Robot researched secure e-mail services to the extent that they actually contacted the ProtonMail developers and asked if it was possible for users to monitor their own e-mail activity in ProtonMail. The ProtonMail developers liked the idea of account access logs so much that they ended up implementing it in their v2.0 release of ProtonMail."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#pycharm",
    "href": "posts/mr_robot_softwares/index.html#pycharm",
    "title": "All Software used in Mr. Robot",
    "section": "PyCharm",
    "text": "PyCharm\nPyCharm is a Python and Django IDE (Integrated Developer Environment), which is a type of code editor software. It is used by Trenton in season 1, episode 4."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#bluesniff",
    "href": "posts/mr_robot_softwares/index.html#bluesniff",
    "title": "All Software used in Mr. Robot",
    "section": "Bluesniff",
    "text": "Bluesniff\nBluesniff is a Bluetooth device discovery tool. In season 1, episode 6 Elliot uses Bluesniff in combination with btscanner and Metasploit when he connects to the computer in a nearby police car via a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#btscanner",
    "href": "posts/mr_robot_softwares/index.html#btscanner",
    "title": "All Software used in Mr. Robot",
    "section": "btscanner",
    "text": "btscanner\nbtscanner is a tool that is included in Kali Linux that extracts as much information as possible about Bluetooth devices without having to pair. In season 1, episode 6 Elliot uses btscanner in combination with Bluesniff and Metasploit when he connects to the computer in a nearby police car using a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#mozilla-firefox",
    "href": "posts/mr_robot_softwares/index.html#mozilla-firefox",
    "title": "All Software used in Mr. Robot",
    "section": "Mozilla Firefox",
    "text": "Mozilla Firefox\nElliot uses Firefox as his default web browser. Trenton uses Firefox in season 2, episode 8."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#vlc-media-player",
    "href": "posts/mr_robot_softwares/index.html#vlc-media-player",
    "title": "All Software used in Mr. Robot",
    "section": "VLC Media Player",
    "text": "VLC Media Player\nVLC Media Player was used in season 2, episode 4 when Elliot and Darlene watched a VHS rip of the fake horror movie Careful Massacre of the Bourgeoisie together. VLC is also used in season 2, episode 8 when fsociety preview the video they are about to upload a leaked FBI conference call about illegal mass surveillance."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#µtorrent",
    "href": "posts/mr_robot_softwares/index.html#µtorrent",
    "title": "All Software used in Mr. Robot",
    "section": "µTorrent",
    "text": "µTorrent\nIn season 2, episode 4 Darlene was downloading a VHS rip of the fake horror movie Careful Massacre of the Bourgeoisie using µTorrent."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#flexispy",
    "href": "posts/mr_robot_softwares/index.html#flexispy",
    "title": "All Software used in Mr. Robot",
    "section": "FlexiSPY",
    "text": "FlexiSPY\nFlexiSPY is a spyware software for Android, iOS and BlackBerry that lets the user monitor all activities on the victims phone. In season 1, episode 3 Tyrell Wellick covertly installs it on a co-worker’s Android phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#kingoroot",
    "href": "posts/mr_robot_softwares/index.html#kingoroot",
    "title": "All Software used in Mr. Robot",
    "section": "KingoRoot",
    "text": "KingoRoot\nKingo Root is used by Tyrell Wellick in season 1, episode 3 to root a co-worker’s Android phone so that he can covertly install the FlexiSPY spyware on the phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#kvm-kernel-based-virtual-machine",
    "href": "posts/mr_robot_softwares/index.html#kvm-kernel-based-virtual-machine",
    "title": "All Software used in Mr. Robot",
    "section": "KVM (Kernel-based Virtual Machine)",
    "text": "KVM (Kernel-based Virtual Machine)\nKVM is a hypervisor, which is a software that can run other operating systems via virtual machines. Elliot uses KVM to virtualize Windows 7 inside of Kali Linux. In season 1, episode 6 Elliot uses KVM to run Metasploit and Metapreter and in season 1, episode 8 he uses KVM to run DeepSound."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#metasploit",
    "href": "posts/mr_robot_softwares/index.html#metasploit",
    "title": "All Software used in Mr. Robot",
    "section": "Metasploit",
    "text": "Metasploit\nMetasploit Framework is a software included in Kali Linux that makes it easier to discover vulnerabilities in networks for penetration testers. Meterpreter is one of several hundreds of payloads that can be run in the Metasploit Framework and it is used in season 1, episode 6. In season 1, episode 6 Elliot uses Metasploit Framwork and Metapreter in combination with btscanner and Bluesniff when he connects to the computer in a nearby police car via a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#framaroot",
    "href": "posts/mr_robot_softwares/index.html#framaroot",
    "title": "All Software used in Mr. Robot",
    "section": "Framaroot",
    "text": "Framaroot\nFramaroot - called RooterFrame in the show - is used by Tyrell Wellick in season 1, episode 3 to root a co-worker’s Android phone so that he can covertly install the FlexiSPY spyware on the phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#supersu",
    "href": "posts/mr_robot_softwares/index.html#supersu",
    "title": "All Software used in Mr. Robot",
    "section": "SuperSU",
    "text": "SuperSU\nSuperSU is an app that managed superuser privileges on rooted Android phones. In season 1, episode 3 Tyrell Wellick covertly installs FlexiSPY - which uses SuperSU to give itself superuser access - on a co-worker’s Android phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/data_imputation/index.html",
    "href": "posts/data_imputation/index.html",
    "title": "Handling Missing Data",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/data_imputation/index.html#handling-missing-data",
    "href": "posts/data_imputation/index.html#handling-missing-data",
    "title": "Handling Missing Data",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/data_imputation/index.html#introduction",
    "href": "posts/data_imputation/index.html#introduction",
    "title": "Handling Missing Data",
    "section": "Introduction:",
    "text": "Introduction:\nMissing data is a common hurdle in data analysis, impacting the reliability of insights drawn from datasets. Python offers a range of solutions to address this issue, some of which we discussed in the earlier weeks. In this notebook, we look into the top three missing data imputation methods in Python—SimpleImputer, KNNImputer, and IterativeImputer from scikit-learn—providing insights into their functionalities and practical considerations. We’ll explore these essential techniques, using the weather dataset.\n# install the libraries for this demonstration\n! pip install dataidea==0.2.5\nfrom dataidea.packages import * \nfrom dataidea.datasets import loadDataset\nfrom dataidea.packages import * imports for us np, pd, plt, etc. loadDataset allows us to load datasets inbuilt in the dataidea library\nweather = loadDataset('weather') \nweather\n\n\n\n\n\n\n\n\nday\n\n\ntemperature\n\n\nwindspead\n\n\nevent\n\n\n\n\n\n\n0\n\n\n01/01/2017\n\n\n32.0\n\n\n6.0\n\n\nRain\n\n\n\n\n1\n\n\n04/01/2017\n\n\nNaN\n\n\n9.0\n\n\nSunny\n\n\n\n\n2\n\n\n05/01/2017\n\n\n28.0\n\n\nNaN\n\n\nSnow\n\n\n\n\n3\n\n\n06/01/2017\n\n\nNaN\n\n\n7.0\n\n\nNaN\n\n\n\n\n4\n\n\n07/01/2017\n\n\n32.0\n\n\nNaN\n\n\nRain\n\n\n\n\n5\n\n\n08/01/2017\n\n\nNaN\n\n\nNaN\n\n\nSunny\n\n\n\n\n6\n\n\n09/01/2017\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n7\n\n\n10/01/2017\n\n\n34.0\n\n\n8.0\n\n\nCloudy\n\n\n\n\n8\n\n\n11/01/2017\n\n\n40.0\n\n\n12.0\n\n\nSunny\n\n\n\n\n\nweather.isna().sum()\nday            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\nLet’s demonstrate how to use the top three missing data imputation methods—SimpleImputer, KNNImputer, and IterativeImputer—using the simple weather dataset.\n# select age from the data\ntemp_wind = weather[['temperature', 'windspead']].copy()\ntemp_wind_imputed = temp_wind.copy()"
  },
  {
    "objectID": "posts/data_imputation/index.html#simpleimputer-from-scikit-learn",
    "href": "posts/data_imputation/index.html#simpleimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "SimpleImputer from scikit-learn:",
    "text": "SimpleImputer from scikit-learn:\n\nUsage: SimpleImputer is a straightforward method for imputing missing values by replacing them with a constant, mean, median, or most frequent value along each column.\nPros:\n\nEasy to use and understand.\nCan handle both numerical and categorical data.\nOffers flexibility with different imputation strategies.\n\nCons:\n\nIt doesn’t consider relationships between features.\nMay not be the best choice for datasets with complex patterns of missingness.\n\nExample:\n\nfrom sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer(strategy='mean')\ntemp_wind_simple_imputed = simple_imputer.fit_transform(temp_wind)\n\ntemp_wind_simple_imputed_df = pd.DataFrame(temp_wind_simple_imputed, columns=temp_wind.columns)\nLet’s have a look at the outcome\ntemp_wind_simple_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.0\n\n\n6.0\n\n\n\n\n1\n\n\n33.2\n\n\n9.0\n\n\n\n\n2\n\n\n28.0\n\n\n8.4\n\n\n\n\n3\n\n\n33.2\n\n\n7.0\n\n\n\n\n4\n\n\n32.0\n\n\n8.4\n\n\n\n\n5\n\n\n33.2\n\n\n8.4\n\n\n\n\n6\n\n\n33.2\n\n\n8.4\n\n\n\n\n7\n\n\n34.0\n\n\n8.0\n\n\n\n\n8\n\n\n40.0\n\n\n12.0"
  },
  {
    "objectID": "posts/data_imputation/index.html#knnimputer-from-scikit-learn",
    "href": "posts/data_imputation/index.html#knnimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "KNNImputer from scikit-learn:",
    "text": "KNNImputer from scikit-learn:\n\nUsage: KNNImputer imputes missing values using k-nearest neighbors, replacing them with the mean value of the nearest neighbors.\nPros:\n\nConsiders relationships between features, making it suitable for datasets with complex patterns of missingness.\nCan handle both numerical and categorical data.\n\nCons:\n\nComputationally expensive for large datasets.\nRequires careful selection of the number of neighbors (k).\n\nExample:\n\nfrom sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\ntemp_wind_knn_imputed = knn_imputer.fit_transform(temp_wind)\n\ntemp_wind_knn_imputed_df = pd.DataFrame(temp_wind_knn_imputed, columns=temp_wind.columns)\nIf we take a look at the outcome\ntemp_wind_knn_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.0\n\n\n6.0\n\n\n\n\n1\n\n\n33.0\n\n\n9.0\n\n\n\n\n2\n\n\n28.0\n\n\n7.0\n\n\n\n\n3\n\n\n33.0\n\n\n7.0\n\n\n\n\n4\n\n\n32.0\n\n\n7.0\n\n\n\n\n5\n\n\n33.2\n\n\n8.4\n\n\n\n\n6\n\n\n33.2\n\n\n8.4\n\n\n\n\n7\n\n\n34.0\n\n\n8.0\n\n\n\n\n8\n\n\n40.0\n\n\n12.0"
  },
  {
    "objectID": "posts/data_imputation/index.html#iterativeimputer-from-scikit-learn",
    "href": "posts/data_imputation/index.html#iterativeimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "IterativeImputer from scikit-learn:",
    "text": "IterativeImputer from scikit-learn:\n\nUsage: IterativeImputer models each feature with missing values as a function of other features and uses that estimate for imputation. It iteratively estimates the missing values.\nPros:\n\nTakes into account relationships between features, making it suitable for datasets with complex missing patterns.\nMore robust than SimpleImputer for handling missing data.\n\nCons:\n\nCan be computationally intensive and slower than SimpleImputer.\nRequires careful tuning of model parameters.\n\nExample:\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niterative_imputer = IterativeImputer()\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\nLet’s take a look at the outcome\ntemp_wind_iterative_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.000000\n\n\n6.000000\n\n\n\n\n1\n\n\n35.773287\n\n\n9.000000\n\n\n\n\n2\n\n\n28.000000\n\n\n3.321648\n\n\n\n\n3\n\n\n33.042537\n\n\n7.000000\n\n\n\n\n4\n\n\n32.000000\n\n\n6.238915\n\n\n\n\n5\n\n\n33.545118\n\n\n7.365795\n\n\n\n\n6\n\n\n33.545118\n\n\n7.365795\n\n\n\n\n7\n\n\n34.000000\n\n\n8.000000\n\n\n\n\n8\n\n\n40.000000\n\n\n12.000000"
  },
  {
    "objectID": "posts/data_imputation/index.html#datawig",
    "href": "posts/data_imputation/index.html#datawig",
    "title": "Handling Missing Data",
    "section": "Datawig:",
    "text": "Datawig:\nDatawig is a library specifically designed for imputing missing values in tabular data using deep learning models.\n# import datawig\n\n# # Impute missing values\n# df_imputed = datawig.SimpleImputer.complete(weather)\nThese top imputation methods offer different trade-offs in terms of computational complexity, handling of missing data patterns, and ease of use. The choice between them depends on the specific characteristics of the dataset and the requirements of the analysis."
  },
  {
    "objectID": "posts/data_imputation/index.html#homework",
    "href": "posts/data_imputation/index.html#homework",
    "title": "Handling Missing Data",
    "section": "Homework",
    "text": "Homework\n\nTry out these techniques for categorical data"
  },
  {
    "objectID": "posts/data_imputation/index.html#credit",
    "href": "posts/data_imputation/index.html#credit",
    "title": "Handling Missing Data",
    "section": "Credit",
    "text": "Credit\n\nDo you seriously want to learn Programming and Data Analysis with Python?\nIf you’re serious about learning Programming, Data Analysis with Python and getting prepared for Data Science roles, I highly encourage you to enroll in my Programming for Data Science Course, which I’ve taught to hundreds of students. Don’t waste your time following disconnected, outdated tutorials\nMy Complete Programming for Data Science Course has everything you need in one place.\nThe course offers:\n\nDuration: Usually 3-4 months\nSessions: Four times a week (one on one)\nLocation: Online or/and at UMF House, Sir Apollo Kagwa Road\n\nWhat you’l learn:\n\nFundamentals of programming\nData manipulation and analysis\nVisualization techniques\nIntroduction to machine learning\nDatabase Management with SQL (optional)\nWeb Development with Django (optional)\n\nBest\nJuma Shafara\nData Scientist, Instructor\njumashafara0@gmail.com / dataideaorg@gmail.com\n+256701520768 / +256771754118"
  },
  {
    "objectID": "posts/fpl_standings/index.html",
    "href": "posts/fpl_standings/index.html",
    "title": "Update on DATAIDEA Fantasy Football League Standings",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nHey Fantasy Football aficionados,\nIt’s time for another exciting update on the DATAIDEA Fantasy Football League! With each matchweek passing by, the competition has been heating up, and the leaderboard has seen its fair share of twists and turns. Let’s dive straight into the latest standings and highlights from the league.\n\nCurrent Standings:\nHere’s how the teams stack up on the leaderboard:\n\n\n\nRank\nTeam & Manager\nGW\nTOT\n\n\n\n\n1\nBARÇA ya KAHUNGYE\n47\n2089\n\n\n2\nAŁbramo FC\n61\n2053\n\n\n3\nLethal Weapon\n53\n2009\n\n\n4\nBlyckfc\n30\n2002\n\n\n5\nBrine FC\n17\n1959\n\n\n6\nJ T\n78\n1958\n\n\n7\nJumaShafara@DATAIDEA\n43\n1951\n\n\n8\nRwenzori\n59\n1949\n\n\n9\nGem FC\n59\n1911\n\n\n10\nGULU UNITED F.C\n38\n1908\n\n\n11\nAJ12\n45\n1887\n\n\n12\nLegends FC\n44\n1823\n\n\n13\nM142 HIMARS\n56\n1804\n\n\n14\nColaz guluz\n42\n1781\n\n\n15\nPaperChaser of Uganda\n34\n1732\n\n\n16\nAFC Adventurer\n39\n1669\n\n\n\n\n\nHighlights:\n\nBARCA ya KAHUNGYE continues to dominate the league, maintaining their top position with an impressive total score of 2042 points.\nAlBramo FC is trailing closely behind in second place, displaying consistent performance throughout the season.\nJumaShafara@DATAIDEA has climbed up the ranks to secure the fifth position, showcasing the competitive spirit of DATAIDEA’s own fantasy football enthusiasts.\n\n\n\nLooking Ahead:\nAs the league progresses, the competition is only expected to intensify further. With managers strategizing and making crucial transfers, every matchweek brings forth new challenges and opportunities for teams to climb the ranks.\nStay tuned for more updates as the DATAIDEA Fantasy Football League unfolds its thrilling saga of goals, assists, and tactical maneuvers.\nUntil next time, may your fantasy team flourish on the virtual pitch!\nBest regards,\nJuma Shafara\nInstructor, DATAIDEA\n+256771754118 / jumashafara0@gmail.com\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/programmer_puns/index.html",
    "href": "posts/programmer_puns/index.html",
    "title": "Puns Only Programmers Will Get",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nWhat are the best puns? Definitely the ones only a specific group of people will understand and enjoy. Especially in the era we’re living in, we need to feel like a family and share “inside” jokes with one another. So, let’s take a look at some of the best programming puns we came upon during the pre-summer times.\n\nWhy do programmers like dark mode?\n\nBecause light attracts bugs.\n\nI used to work as a programmer for autocorrect…\n\nThen they fried me for no raisin.\n\nWhat do NASA programmers do on the weekends?\n\nThey hit the space bar.\n\nWhy did the programmer quit his job?\n\nBecause he didn’t get arrays.\n\nWhat was the SNES programmers’ favorite drink?\n\nSprite\n\nWhat do programmers do when they’re hungry?\n\nThey grab a byte.\n\nWhy couldn’t the programmer dance to the song?\n\nBecause he didn’t get the… algo-rhythm…\n\nWhat you call it when computer programmers make fun of each other?\n\nCyber boolean…\n\nI am now a successful programmer…\n\nBut back in the days I was a noobgrammer.\n\nWhy do programmers always mix up Halloween and Christmas?\n\nBecause Oct 31 equals Dec 25.\n\nWhy did the programmer get a huge telephone bill?\n\nBecause his program was CALLING a lot of subroutines.\n\nWhat do Spanish programmers code in?\n\nSí ++\n\nWhich way did the programmer go?\n\nHe went data way!\n\nWhy was the programmer always running into walls?\n\nHe couldn’t see sharp.\n\nHow programmers curse?\n\nOh shift!\n\nWhy do programmers make good politicians?\n\nTheir goto is to switch statements.\n\nHow did the programmer lose weight?\n\nHe switched to a byte-sized diet.\n\nI almost bought a huge library out of old computer programming books…\n\nBut the ascii price was way too high.\n\nWhat’s a Jedi’s favorite programming language?\n\nJabbaScript…\n\n\n\nWant to learn programming and become an expert?\nIf you’re serious about learning Programming and getting a job as a Developer, I highly encourage you to enroll in my programming courses for Python, JavaScript, Data Science and Web Development.\nDon’t waste your time following disconnected, outdated tutorials. I can teach you all that you need to kickstart your career.\nContact me at +256771754118 or jumashafara@proton.me\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html",
    "href": "posts/top_5_operating_systems/index.html",
    "title": "To 5 Operating Systems",
    "section": "",
    "text": "Thumbnail\nOperating systems are the backbone of modern computing, providing the fundamental software foundation for computers, smartphones, servers, and more. Here’s a comprehensive overview of five prominent operating systems that have made significant impacts in various domains."
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html#windows",
    "href": "posts/top_5_operating_systems/index.html#windows",
    "title": "To 5 Operating Systems",
    "section": "Windows",
    "text": "Windows\n\nOverview\nDeveloped by Microsoft, Windows is one of the most widely used operating systems across PCs, servers, and embedded systems. Known for its user-friendly interface and extensive software compatibility, Windows offers a variety of versions tailored for different devices and use cases.\n\n\nFeatures\n\nGUI: Graphical User Interface for intuitive navigation.\nCompatibility: Broad support for software and hardware.\nSecurity: Regular updates and security features."
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html#macos",
    "href": "posts/top_5_operating_systems/index.html#macos",
    "title": "To 5 Operating Systems",
    "section": "macOS",
    "text": "macOS\n\nOverview\nExclusive to Apple’s hardware, macOS offers a sleek and intuitive user experience. It is known for its stability, design aesthetics, and seamless integration with other Apple devices and services.\n\n\nFeatures\n\nIntegration: Seamless integration with other Apple devices.\nPerformance: Optimized for Mac hardware.\nSecurity: Built-in encryption and privacy features."
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html#linux",
    "href": "posts/top_5_operating_systems/index.html#linux",
    "title": "To 5 Operating Systems",
    "section": "Linux",
    "text": "Linux\n\nOverview\nLinux is an open-source operating system available in various distributions (distros), catering to diverse user needs. Its flexibility, stability, and robustness have made it a favorite among developers, server administrators, and tech enthusiasts.\n\n\nFeatures\n\nOpen Source: Community-driven development and customization.\nVariety: Diverse distributions catering to different needs.\nStability: Known for reliability and performance."
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html#android",
    "href": "posts/top_5_operating_systems/index.html#android",
    "title": "To 5 Operating Systems",
    "section": "Android",
    "text": "Android\n\nOverview\nDeveloped by Google, Android is the leading operating system for mobile devices globally. Its open-source nature, customizable interface, and extensive app ecosystem have made it a dominant force in the mobile market.\n\n\nFeatures\n\nCustomization: Ability to personalize interface and features.\nApp Ecosystem: Extensive library of applications.\nIntegration: Seamless integration with Google services."
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html#iosipados",
    "href": "posts/top_5_operating_systems/index.html#iosipados",
    "title": "To 5 Operating Systems",
    "section": "iOS/iPadOS",
    "text": "iOS/iPadOS\n\nOverview\nExclusive to Apple’s mobile devices, iOS and iPadOS are renowned for their smooth performance, security features, and seamless integration within the Apple ecosystem.\n\n\nFeatures\n\nPerformance: Smooth and efficient operation on Apple devices.\nSecurity: Robust security measures and privacy controls.\nEcosystem Integration: Seamless integration with other Apple devices.\n\nIn conclusion, these operating systems showcase diversity in terms of architecture, platform support, and features, catering to the varied needs of users across different devices and industries.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/time_series_forecasting/index.html",
    "href": "posts/time_series_forecasting/index.html",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 3 of our Time Series Analysis Series, Part 2 introduces you to doing time series analysis. The link to part 2 is here\nForecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting\n\nStep 1: Understand the time series characteristics like trend, seasonality etc\nStep 2: Do the analysis and identify the best method to make the time series stationary\nStep 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back\nStep 4: Based on data analysis choose the appropriate model for time series forecasting\nStep 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.\nStep 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.\nStep 7: At the end we can do the future forecasting and get the future forecasted values in original scale."
  },
  {
    "objectID": "posts/time_series_forecasting/index.html#time-series-forecasting",
    "href": "posts/time_series_forecasting/index.html#time-series-forecasting",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 3 of our Time Series Analysis Series, Part 2 introduces you to doing time series analysis. The link to part 2 is here\nForecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting\n\nStep 1: Understand the time series characteristics like trend, seasonality etc\nStep 2: Do the analysis and identify the best method to make the time series stationary\nStep 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back\nStep 4: Based on data analysis choose the appropriate model for time series forecasting\nStep 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.\nStep 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.\nStep 7: At the end we can do the future forecasting and get the future forecasted values in original scale."
  },
  {
    "objectID": "posts/time_series_forecasting/index.html#models-used-for-time-series-forecasting",
    "href": "posts/time_series_forecasting/index.html#models-used-for-time-series-forecasting",
    "title": "Time Series Forecasting",
    "section": "Models Used For Time Series Forecasting",
    "text": "Models Used For Time Series Forecasting\n\nAutoregression (AR)\nMoving Average (MA)\nAutoregressive Moving Average (ARMA)\nAutoregressive Integrated Moving Average (ARIMA)\nSeasonal Autoregressive Integrated Moving-Average (SARIMA)\nSeasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)\nVector Autoregression (VAR)\nVector Autoregression Moving-Average (VARMA)\nVector Autoregression Moving-Average with Exogenous Regressors (VARMAX)\nSimple Exponential Smoothing (SES)\nHolt Winter’s Exponential Smoothing (HWES)\n\nNext part of this article we are going to analyze and forecast air passengers time series data using ARIMA model. Brief introduction of ARIMA model is as below\n[ad]"
  },
  {
    "objectID": "posts/time_series_forecasting/index.html#arima",
    "href": "posts/time_series_forecasting/index.html#arima",
    "title": "Time Series Forecasting",
    "section": "ARIMA",
    "text": "ARIMA\n\nARIMA stands for Auto-Regressive Integrated Moving Averages. It is actually a combination of AR and MA model.\nARIMA has three parameters ‘p’ for the order of Auto-Regressive (AR) part, ‘q’ for the order of Moving Average (MA) part and ‘d’ for the order of integrated part.\n\n\nAuto-Regressive (AR) Model:\n\nAs the name indicates, its the regression of the variables against itself. In this model linear combination of the past values are used to forecast the future values.\nTo figure out the order of AR model we will use PACF function\n\n\n\nIntegration(I):\n\nUses differencing of observations (subtracting an observation from observation at the previous time step) in order to make the time series stationary. Differencing involves the subtraction of the current values of a series with its previous values \\(d\\) number of times.\nMost of the time value of \\(d = 1\\), means first order of difference.\n\n\n\nMoving Average (MA) Model:\n\nRather than using past values of the forecast variable in a regression, a moving average model uses linear combination of past forecast errors\nTo figure out the order of MA model we will use ACF function\n\n[ad]"
  },
  {
    "objectID": "posts/time_series_intro/index.html",
    "href": "posts/time_series_intro/index.html",
    "title": "What is Time Series",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nAny data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temperature reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable.\nSales forecasting time series with shampoo sales for every month will look like this,\n\n\n\nShampoo_Sales\n\n\nIn above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.\nObjective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.\n[ad]"
  },
  {
    "objectID": "posts/time_series_intro/index.html#what-is-time-series",
    "href": "posts/time_series_intro/index.html#what-is-time-series",
    "title": "What is Time Series",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nAny data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temperature reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable.\nSales forecasting time series with shampoo sales for every month will look like this,\n\n\n\nShampoo_Sales\n\n\nIn above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.\nObjective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.\n[ad]"
  },
  {
    "objectID": "posts/time_series_intro/index.html#time-series-characteristics",
    "href": "posts/time_series_intro/index.html#time-series-characteristics",
    "title": "What is Time Series",
    "section": "Time Series Characteristics",
    "text": "Time Series Characteristics\nMean, standard deviation and seasonality defines different characteristics of the time series.\n\n\n\nTime_Series_Characteristics\n\n\nImportant characteristics of the time series are as below\n\nTrend\nTrend represent the change in dependent variables with respect to time from start to end. In case of increasing trend dependent variable will increase with time and vice versa. It’s not necessary to have definite trend in time series, we can have a single time series with increasing and decreasing trend. In short trend represent the varying mean of time series data.\n\n\n\nTrend\n\n\n\n\nSeasonality\nIf observations repeats after fixed time interval then they are referred as seasonal observations. These seasonal changes in data can occur because of natural events or man-made events. For example every year warm cloths sales increases just before winter season. So seasonality represent the data variations at fixed intervals.\n\n\n\nSeasonality\n\n\n\n\nIrregularities\nThis is also called as noise. Strange dips and jump in the data are called as irregularities. These fluctuations are caused by uncontrollable events like earthquakes, wars, flood, pandemic etc. For example because of COVID-19 pandemic there is huge demand for hand sanitizers and masks.\n\n\n\nIrregularities\n\n\n\n\nCyclicity\nCyclicity occurs when observations in the series repeats in random pattern. Note that if there is any fixed pattern then it becomes seasonality, in case of cyclicity observations may repeat after a week, months or may be after a year. These kinds of patterns are much harder to predict.\n\n\n\nCyclicity\n\n\nTime series data which has above characteristics is called as ‘Non-Stationary Data’. For any analysis on time series data we must convert it to ‘Stationary Data’\nThe general guideline is to estimate the trend and seasonality in the time series, and then make the time series stationary for data modeling. In data modeling step statistical techniques are used for time series analysis and forecasting. Once we have the predictions, in the final step forecasted values converted into the original scale by applying trend and seasonality constraints back.\n[ad]\n\n\n\n\n\nPart 2 of this Time Series Analysis series will introduce you to doing time series analysis. The link to part 2 is here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Movies and Data Science",
    "section": "",
    "text": "X-Men ’97 Season 1 Ending Explained and What’s Next for Season 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/x_men_97_ending/index.html",
    "href": "posts/x_men_97_ending/index.html",
    "title": "X-Men ’97 Season 1 Ending Explained and What’s Next for Season 2",
    "section": "",
    "text": "IMDb"
  },
  {
    "objectID": "posts/x_men_97_ending/index.html#x-men-97-season-1-ending-explained-and-whats-next-for-season-2",
    "href": "posts/x_men_97_ending/index.html#x-men-97-season-1-ending-explained-and-whats-next-for-season-2",
    "title": "X-Men ’97 Season 1 Ending Explained and What’s Next for Season 2",
    "section": "X-Men ’97: Season 1 Ending Explained and What’s Next for Season 2",
    "text": "X-Men ’97: Season 1 Ending Explained and What’s Next for Season 2\n\nThe Main Villain of Season 2 May Have Just Been Revealed\n\nWarning: This article contains full spoilers for X-Men ’97: Season 1, Episode 10! Be sure to check out IGN’s review of the Season 1 finale if you haven’t already.\nX-Men ’97 has concluded its first season on Disney+, bringing significant changes in the wake of the final battle with Bastion. Although Operation: Zero Tolerance has been thwarted and the world saved from annihilation, the X-Men are now missing and presumed dead. As the world recovers, new conflicts are emerging across time.\n\nThe X-Men Save the World (Again)\nThe finale’s climax paints a dire picture for humanity. Despite Professor Xavier hijacking Magneto’s mind to undo the EMP attack, humanity still faces extinction as Bastion transforms Asteroid M into a deadly projectile. Bastion’s plan is reminiscent of Ultron’s in Avengers: Age of Ultron.\nBastion is ultimately destroyed after rejecting Cyclops’ act of compassion and being sucked into a black hole created by Asteroid M’s damaged core. Magneto, regaining consciousness just in time, prevents Asteroid M from crashing into Earth. However, the X-Men and Magneto are presumed dead after the asteroid explodes.\nThis heroic act sets a new tone for human/mutant relations in Season 2. Despite Bastion discrediting the X-Men and revealing the truth about Xavier’s death, the X-Men and Magneto’s sacrifice have shown world leaders their willingness to protect humanity. Anti-mutant hysteria persists, but the X-Men’s heroism is a powerful counter-narrative.\n\n\nThe Marvel Cameos in Episode 10\nEpisode 10 features numerous superhero cameos, including:\n\nCaptain America\nIron Man\nBlack Panther (T’Chaka)\nDaredevil\nCloak and Dagger\nDoctor Strange\nSilver Samurai\nOmega Red\nDawnstar\nPeter Parker\nMary Jane Watson\n\nFormer showrunner Beau DeMayo suggested that many heroes are off-world, dealing with a crisis involving the Shi’ar and Kree empires.\n\n\nSaved by Time Travel\nThe final scenes reveal that Magneto and the X-Men didn’t perish; they were transported across time. Most of them landed in ancient Egypt 5000 years ago, while Cyclops and Jean Grey found themselves 2000 years in the future.\nThis time travel may have been accidental, triggered by Bastion absorbing Cable’s time-traveling techno-organic arm. With Bastion’s destruction, time holes may have opened, pulling the X-Men into different eras.\nCable, now stranded in the present, could lead to the formation of X-Force, filling the void left by the X-Men.\n\n\nMeeting Apocalypse in Ancient Egypt\nMagneto, Rogue, Nightcrawler, Beast, and Xavier find themselves in Egypt in 3000 BC, where they encounter En Sabah Nur, the young Apocalypse. This ancient mutant, often depicted as one of the first mutants, has a futuristic temple in the background, hinting at his battle with the time-traveler Pharaoh Rama-Tut.\nRama-Tut, an alias of Kang the Conqueror, might play a significant role in Season 2. The conflict between Apocalypse and Rama-Tut, with the X-Men’s involvement, could alter En Sabah Nur’s destiny, potentially preventing him from becoming a villain.\n\n\nCyclops, Jean Grey, and Clan Askani\nCyclops and Jean Grey arrive in 3060 AD, encountering Clan Askani led by Mother Askani. They meet a younger Nathan Summers, their son, allowing them to regain lost time. This plot mirrors the 1994 comic series The Adventures of Cyclops and Phoenix, where Scott and Jean were psychically transported to the future to raise Nathan.\nClan Askani opposes Apocalypse’s rule, indicating a connected storyline across time with Apocalypse as the central villain.\n\n\nThe Fate of Wolverine and the Other X-Men\nWolverine, Storm, and Morph are missing, leaving their whereabouts unknown. Given their popularity, it’s unlikely they died on Asteroid M. They may have been displaced in time as well.\nWolverine, injured with adamantium ripped from his bones, might have to recover physically and psychologically. His healing factor might be compromised, and he may discover his bone claws. Wolverine could face villains like Lady Deathstrike and Cyber while navigating his new reality.\n\n\nX-Men ’97’s Post-Credits Scene Explained\nThe post-credits scene shows Apocalypse on Genosha, mourning mutant losses and holding a Gambit card, suggesting Gambit might be resurrected as his Horseman of Death. Apocalypse’s presence across past, future, and present hints at a central role in Season 2.\nSeason 2 is expected to delve into the connections across time and Apocalypse’s influence, likely returning in 2025.\nFor more on X-Men ’97, explore why the series takes a minimalist approach with Wolverine and stay updated on every Marvel movie and series in development."
  },
  {
    "objectID": "posts/x_men_97_ending_explained/index.html",
    "href": "posts/x_men_97_ending_explained/index.html",
    "title": "X-Men ’97 Season 1 Ending Explained and What’s Next for Season 2",
    "section": "",
    "text": "IMDb"
  },
  {
    "objectID": "posts/x_men_97_ending_explained/index.html#x-men-97-season-1-ending-explained-and-whats-next-for-season-2",
    "href": "posts/x_men_97_ending_explained/index.html#x-men-97-season-1-ending-explained-and-whats-next-for-season-2",
    "title": "X-Men ’97 Season 1 Ending Explained and What’s Next for Season 2",
    "section": "X-Men ’97: Season 1 Ending Explained and What’s Next for Season 2",
    "text": "X-Men ’97: Season 1 Ending Explained and What’s Next for Season 2\n\nThe Main Villain of Season 2 May Have Just Been Revealed\n\nWarning: This article contains full spoilers for X-Men ’97: Season 1, Episode 10! Be sure to check out IGN’s review of the Season 1 finale if you haven’t already.\nX-Men ’97 has concluded its first season on Disney+, bringing significant changes in the wake of the final battle with Bastion. Although Operation: Zero Tolerance has been thwarted and the world saved from annihilation, the X-Men are now missing and presumed dead. As the world recovers, new conflicts are emerging across time.\n\nThe X-Men Save the World (Again)\nThe finale’s climax paints a dire picture for humanity. Despite Professor Xavier hijacking Magneto’s mind to undo the EMP attack, humanity still faces extinction as Bastion transforms Asteroid M into a deadly projectile. Bastion’s plan is reminiscent of Ultron’s in Avengers: Age of Ultron.\nBastion is ultimately destroyed after rejecting Cyclops’ act of compassion and being sucked into a black hole created by Asteroid M’s damaged core. Magneto, regaining consciousness just in time, prevents Asteroid M from crashing into Earth. However, the X-Men and Magneto are presumed dead after the asteroid explodes.\nThis heroic act sets a new tone for human/mutant relations in Season 2. Despite Bastion discrediting the X-Men and revealing the truth about Xavier’s death, the X-Men and Magneto’s sacrifice have shown world leaders their willingness to protect humanity. Anti-mutant hysteria persists, but the X-Men’s heroism is a powerful counter-narrative.\n\n\n\n\n\n\nThe Marvel Cameos in Episode 10\nEpisode 10 features numerous superhero cameos, including:\n\nCaptain America\nIron Man\nBlack Panther (T’Chaka)\nDaredevil\nCloak and Dagger\nDoctor Strange\nSilver Samurai\nOmega Red\nDawnstar\nPeter Parker\nMary Jane Watson\n\nFormer showrunner Beau DeMayo suggested that many heroes are off-world, dealing with a crisis involving the Shi’ar and Kree empires.\n\n\nSaved by Time Travel\nThe final scenes reveal that Magneto and the X-Men didn’t perish; they were transported across time. Most of them landed in ancient Egypt 5000 years ago, while Cyclops and Jean Grey found themselves 2000 years in the future.\nThis time travel may have been accidental, triggered by Bastion absorbing Cable’s time-traveling techno-organic arm. With Bastion’s destruction, time holes may have opened, pulling the X-Men into different eras.\nCable, now stranded in the present, could lead to the formation of X-Force, filling the void left by the X-Men.\n\n\n\n\n\n\nMeeting Apocalypse in Ancient Egypt\nMagneto, Rogue, Nightcrawler, Beast, and Xavier find themselves in Egypt in 3000 BC, where they encounter En Sabah Nur, the young Apocalypse. This ancient mutant, often depicted as one of the first mutants, has a futuristic temple in the background, hinting at his battle with the time-traveler Pharaoh Rama-Tut.\nRama-Tut, an alias of Kang the Conqueror, might play a significant role in Season 2. The conflict between Apocalypse and Rama-Tut, with the X-Men’s involvement, could alter En Sabah Nur’s destiny, potentially preventing him from becoming a villain.\n\n\nCyclops, Jean Grey, and Clan Askani\nCyclops and Jean Grey arrive in 3060 AD, encountering Clan Askani led by Mother Askani. They meet a younger Nathan Summers, their son, allowing them to regain lost time. This plot mirrors the 1994 comic series The Adventures of Cyclops and Phoenix, where Scott and Jean were psychically transported to the future to raise Nathan.\nClan Askani opposes Apocalypse’s rule, indicating a connected storyline across time with Apocalypse as the central villain.\n\n\n\n\n\n\nThe Fate of Wolverine and the Other X-Men\nWolverine, Storm, and Morph are missing, leaving their whereabouts unknown. Given their popularity, it’s unlikely they died on Asteroid M. They may have been displaced in time as well.\nWolverine, injured with adamantium ripped from his bones, might have to recover physically and psychologically. His healing factor might be compromised, and he may discover his bone claws. Wolverine could face villains like Lady Deathstrike and Cyber while navigating his new reality.\n\n\nX-Men ’97’s Post-Credits Scene Explained\nThe post-credits scene shows Apocalypse on Genosha, mourning mutant losses and holding a Gambit card, suggesting Gambit might be resurrected as his Horseman of Death. Apocalypse’s presence across past, future, and present hints at a central role in Season 2.\nSeason 2 is expected to delve into the connections across time and Apocalypse’s influence, likely returning in 2025.\nFor more on X-Men ’97, explore why the series takes a minimalist approach with Wolverine and stay updated on every Marvel movie and series in development."
  }
]